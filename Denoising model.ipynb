{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18528fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, MaxPooling2D, Input, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import History\n",
    "import netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c43fb9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padd_audio(audio, max_duration=5, sample_rate=16000):\n",
    "    \"\"\"This function take an audio and padd that audio with zeros\"\"\"\n",
    "    \n",
    "    max_length = max_duration * sample_rate\n",
    "    padding_needed = max_length - len(audio)\n",
    "    pad_left = padding_needed // 2\n",
    "    pad_right = padding_needed - pad_left\n",
    "    \n",
    "    return np.pad(audio, (pad_left, pad_right), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6fec962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_audio(original_audio_path, noise_audio_path, sample_rate=16000):\n",
    "    \"\"\"This function take an original audio path and noise audio path and mix it together\"\"\"\n",
    "    \n",
    "    # Load the original audio\n",
    "    original_audio, sr = librosa.load(original_audio_path, sr=sample_rate)\n",
    "    \n",
    "    #Padd original audio\n",
    "    original_audio = padd_audio(original_audio, sample_rate=sample_rate)\n",
    "    \n",
    "    # Load the noise audio\n",
    "    noise_audio, sr_noise = librosa.load(noise_audio_path, sr=sample_rate)\n",
    "    \n",
    "    # Repeat the noise audio\n",
    "    noise_audio = np.tile(noise_audio, int(np.ceil(len(original_audio) / len(noise_audio))))\n",
    "\n",
    "    # Trim the repeated noise audio to match the length of the original audio\n",
    "    noise_audio = noise_audio[:len(original_audio)]\n",
    "    \n",
    "    return original_audio + noise_audio/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66387d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_audio(original_audio_path, sample_rate=16000):\n",
    "    \"\"\"This function take an original audio path and return padded audio\"\"\"\n",
    "    \n",
    "    # Load the original audio\n",
    "    original_audio, sr = librosa.load(original_audio_path, sr=sample_rate)\n",
    "    \n",
    "    #Padd original audio\n",
    "    return padd_audio(original_audio, sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89d2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randmly chosen noise for each audio\n",
    "def combine_audio_with_noise(original_audio_dir, noise_audio_dir):\n",
    "    combination_dict = {}\n",
    "    noise_audios = os.listdir(noise_audio_dir)\n",
    "    original_audios = os.listdir(original_audio_dir)\n",
    "    \n",
    "    for original_audio in original_audios:\n",
    "        noise_audio = np.random.choice(noise_audios)\n",
    "        combination_dict[os.path.join(original_audio_dir, original_audio)] = os.path.join(noise_audio_dir, noise_audio)\n",
    "        \n",
    "    return combination_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fcecf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_stft(audio, n_fft=1199, hop_length_fft=304):\n",
    "    # STFT transformation\n",
    "    stftaudio = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length_fft)\n",
    "\n",
    "    # Extract magnitude and phase\n",
    "    stftaudio_magnitude, stftaudio_phase = librosa.magphase(stftaudio)\n",
    "\n",
    "    # Convert magnitude to dB\n",
    "    stftaudio_magnitude_db = librosa.amplitude_to_db(stftaudio_magnitude, ref=np.max)\n",
    "    \n",
    "    return stftaudio_magnitude_db, stftaudio_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cda225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft_to_audio(stftaudio_magnitude_db, stftaudio_phase, hop_length_fft=304):\n",
    "# Convert dB back to amplitude\n",
    "    stftaudio_magnitude_rev = librosa.db_to_amplitude(stftaudio_magnitude_db, ref=1.0)\n",
    "\n",
    "    # Reconstruct the STFT complex matrix\n",
    "    audio_reverse_stft = stftaudio_magnitude_rev * stftaudio_phase\n",
    "\n",
    "    # Inverse STFT to get back the audio signal\n",
    "    audio_reconstruct = librosa.istft(audio_reverse_stft, hop_length=hop_length_fft)\n",
    "\n",
    "    return audio_reconstruct / np.max(np.abs(audio_reconstruct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "861a3435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of normalization function using global min and max values\n",
    "def normalize(stft, global_min, global_max):\n",
    "    return (stft - global_min) / (global_max - global_min)\n",
    "\n",
    "# Example of denormalization function using global min and max values\n",
    "def denormalize(normalized_stft, global_min, global_max):\n",
    "    return normalized_stft * (global_max - global_min) + global_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa94b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "        plt.plot(history.history[string])\n",
    "        plt.plot(history.history['val_'+string])\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(string)\n",
    "        plt.legend([string, 'val_'+string])\n",
    "        plt.title(string+' through epochs')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cabcf8",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59bfacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16000  # Sampling rate   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f92181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_noise_pairs = combine_audio_with_noise(os.path.join(os.getcwd(), 'Dataset'), os.path.join(os.getcwd(), 'Noise'))\n",
    "\n",
    "#Getting all the noisy and clean audio matrices\n",
    "noisy_audios = np.zeros(len(audio_noise_pairs), dtype=object)\n",
    "clean_audios = np.zeros(len(audio_noise_pairs), dtype=object)\n",
    "\n",
    "for index, (audio_dir, noise_dir) in enumerate(audio_noise_pairs.items()):\n",
    "    noisy_audios[index] = mix_audio(audio_dir, noise_dir, sample_rate=16000)\n",
    "    clean_audios[index] = get_clean_audio(audio_dir, sample_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b5bbe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting STFT data for learning process\n",
    "noisy_audios_stft = [audio_to_stft(audio)[0] for audio in noisy_audios]\n",
    "noisy_audios_stft_phase = [audio_to_stft(audio)[1] for audio in noisy_audios]\n",
    "clean_audios_stft = [audio_to_stft(audio)[0] for audio in clean_audios]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e2c3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data for training 80%, validation 10% and test set 10%\n",
    "split_ratio1 = 0.8\n",
    "split_ratio2 = 0.9\n",
    "split_index1 = int(len(noisy_audios_stft) * split_ratio1)\n",
    "split_index2 = int(len(noisy_audios_stft) * split_ratio2)\n",
    "\n",
    "X_train = noisy_audios_stft[:split_index1]\n",
    "X_val = noisy_audios_stft[split_index1:split_index2]\n",
    "X_test = noisy_audios_stft[split_index2:]\n",
    "\n",
    "y_train = clean_audios_stft[:split_index1]\n",
    "y_val = clean_audios_stft[split_index1:split_index2]\n",
    "y_test = clean_audios_stft[split_index2:]\n",
    "\n",
    "phase_train = noisy_audios_stft_phase[:split_index1]\n",
    "phase_val = noisy_audios_stft_phase[split_index1:split_index2]\n",
    "phase_test = noisy_audios_stft_phase[split_index2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "717c6024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the global min and max values from both the noisy and clean STFT data\n",
    "global_min = min(np.min(X_train), np.min(y_train))\n",
    "global_max = max(np.max(X_train), np.max(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bbecd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize data based on the data from training set\n",
    "X_train_normalized = normalize(X_train, global_min, global_max)\n",
    "X_val_normalized = normalize(X_val, global_min, global_max)\n",
    "X_test_normalized = normalize(X_test, global_min, global_max)\n",
    "\n",
    "y_train_normalized = normalize(y_train, global_min, global_max)\n",
    "y_val_normalized = normalize(y_val, global_min, global_max)\n",
    "y_test_normalized = normalize(y_test, global_min, global_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ba886f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform data to tensors: sample x dimensions(n x n) x channel(1)\n",
    "X_train_normalized = X_train_normalized[..., np.newaxis]\n",
    "X_val_normalized = X_val_normalized[..., np.newaxis]\n",
    "X_test_normalized = X_test_normalized[..., np.newaxis]\n",
    "\n",
    "y_train_normalized = y_train_normalized[..., np.newaxis]\n",
    "y_val_normalized = y_val_normalized[..., np.newaxis]\n",
    "y_test_normalized = y_test_normalized[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d8a1f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "accc8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X, y, batch_size=16, epochs=100):\n",
    "    assert len(X) == len(y), \"The length of X and y must be the same\"\n",
    "    \n",
    "    # Iterate through each epoch\n",
    "    for epoch in range(epochs):\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        for start in range(0, len(X), batch_size):\n",
    "            end = min(start + batch_size, len(X))\n",
    "            batch_X = X_shuffled[start:end]\n",
    "            batch_y = y_shuffled[start:end]\n",
    "            \n",
    "            yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "300927a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "def encoder(inputs):\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    encoded = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005))(x)\n",
    "    return encoded\n",
    "\n",
    "# Decoder\n",
    "def decoder(encoded):\n",
    "    x = Conv2DTranspose(256, (3, 3), strides=(2, 2), activation='relu', padding='same', kernel_regularizer=l2(0.0005))(encoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Conv2DTranspose(128, (3, 3), strides=(2, 2), activation='relu', padding='same', kernel_regularizer=l2(0.0005))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), activation='relu', padding='same', kernel_regularizer=l2(0.0005))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same', kernel_regularizer=l2(0.0005))(x)  # Single channel output\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80f88407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 600, 264, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 600, 264, 64)      640       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 600, 264, 64)     256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 300, 132, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 300, 132, 64)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 300, 132, 128)     73856     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300, 132, 128)    512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 150, 66, 128)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 150, 66, 128)      0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 150, 66, 256)      295168    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 150, 66, 256)     1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 75, 33, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 75, 33, 256)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 75, 33, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 150, 66, 256)     1179904   \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 150, 66, 256)     1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 150, 66, 256)      0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 300, 132, 128)    295040    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 300, 132, 128)    512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 300, 132, 128)     0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 600, 264, 64)     73792     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 600, 264, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 600, 264, 64)      0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 600, 264, 1)       577       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,102,721\n",
      "Trainable params: 3,100,929\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5214 \n",
      "Epoch 1: val_loss improved from inf to 0.34881, saving model to best_model.h5\n",
      "34/34 [==============================] - 586s 17s/step - loss: 0.5214 - val_loss: 0.3488 - lr: 6.2500e-04\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2212 \n",
      "Epoch 2: val_loss improved from 0.34881 to 0.16249, saving model to best_model.h5\n",
      "34/34 [==============================] - 678s 20s/step - loss: 0.2212 - val_loss: 0.1625 - lr: 6.2500e-04\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1138 \n",
      "Epoch 3: val_loss improved from 0.16249 to 0.09517, saving model to best_model.h5\n",
      "34/34 [==============================] - 689s 20s/step - loss: 0.1138 - val_loss: 0.0952 - lr: 6.2500e-04\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0722 \n",
      "Epoch 4: val_loss improved from 0.09517 to 0.07575, saving model to best_model.h5\n",
      "34/34 [==============================] - 695s 20s/step - loss: 0.0722 - val_loss: 0.0758 - lr: 6.2500e-04\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0581 \n",
      "Epoch 5: val_loss improved from 0.07575 to 0.06716, saving model to best_model.h5\n",
      "34/34 [==============================] - 683s 20s/step - loss: 0.0581 - val_loss: 0.0672 - lr: 6.2500e-04\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0566 \n",
      "Epoch 6: val_loss did not improve from 0.06716\n",
      "34/34 [==============================] - 690s 20s/step - loss: 0.0566 - val_loss: 0.0939 - lr: 6.2500e-04\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0620 \n",
      "Epoch 7: val_loss did not improve from 0.06716\n",
      "34/34 [==============================] - 697s 21s/step - loss: 0.0620 - val_loss: 0.0768 - lr: 6.2500e-04\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0494 \n",
      "Epoch 8: val_loss did not improve from 0.06716\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "34/34 [==============================] - 687s 20s/step - loss: 0.0494 - val_loss: 0.0703 - lr: 6.2500e-04\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0409 \n",
      "Epoch 9: val_loss improved from 0.06716 to 0.06558, saving model to best_model.h5\n",
      "34/34 [==============================] - 689s 20s/step - loss: 0.0409 - val_loss: 0.0656 - lr: 3.1250e-04\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0371 \n",
      "Epoch 10: val_loss improved from 0.06558 to 0.05981, saving model to best_model.h5\n",
      "34/34 [==============================] - 697s 20s/step - loss: 0.0371 - val_loss: 0.0598 - lr: 3.1250e-04\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0347 \n",
      "Epoch 11: val_loss improved from 0.05981 to 0.05822, saving model to best_model.h5\n",
      "34/34 [==============================] - 693s 20s/step - loss: 0.0347 - val_loss: 0.0582 - lr: 3.1250e-04\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0317 \n",
      "Epoch 12: val_loss improved from 0.05822 to 0.05807, saving model to best_model.h5\n",
      "34/34 [==============================] - 687s 20s/step - loss: 0.0317 - val_loss: 0.0581 - lr: 3.1250e-04\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0302 \n",
      "Epoch 13: val_loss improved from 0.05807 to 0.05420, saving model to best_model.h5\n",
      "34/34 [==============================] - 692s 20s/step - loss: 0.0302 - val_loss: 0.0542 - lr: 3.1250e-04\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0285 \n",
      "Epoch 14: val_loss improved from 0.05420 to 0.05228, saving model to best_model.h5\n",
      "34/34 [==============================] - 699s 20s/step - loss: 0.0285 - val_loss: 0.0523 - lr: 3.1250e-04\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0309 \n",
      "Epoch 15: val_loss did not improve from 0.05228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 687s 20s/step - loss: 0.0309 - val_loss: 0.0555 - lr: 3.1250e-04\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0278 \n",
      "Epoch 16: val_loss did not improve from 0.05228\n",
      "34/34 [==============================] - 629s 18s/step - loss: 0.0278 - val_loss: 0.0532 - lr: 3.1250e-04\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0260 \n",
      "Epoch 17: val_loss improved from 0.05228 to 0.05015, saving model to best_model.h5\n",
      "34/34 [==============================] - 619s 18s/step - loss: 0.0260 - val_loss: 0.0501 - lr: 3.1250e-04\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0251 \n",
      "Epoch 18: val_loss improved from 0.05015 to 0.04864, saving model to best_model.h5\n",
      "34/34 [==============================] - 641s 19s/step - loss: 0.0251 - val_loss: 0.0486 - lr: 3.1250e-04\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0239 \n",
      "Epoch 19: val_loss improved from 0.04864 to 0.04584, saving model to best_model.h5\n",
      "34/34 [==============================] - 644s 19s/step - loss: 0.0239 - val_loss: 0.0458 - lr: 3.1250e-04\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0227 \n",
      "Epoch 20: val_loss improved from 0.04584 to 0.04238, saving model to best_model.h5\n",
      "34/34 [==============================] - 655s 19s/step - loss: 0.0227 - val_loss: 0.0424 - lr: 3.1250e-04\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0224 \n",
      "Epoch 21: val_loss improved from 0.04238 to 0.03926, saving model to best_model.h5\n",
      "34/34 [==============================] - 647s 19s/step - loss: 0.0224 - val_loss: 0.0393 - lr: 3.1250e-04\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0215 \n",
      "Epoch 22: val_loss improved from 0.03926 to 0.03859, saving model to best_model.h5\n",
      "34/34 [==============================] - 656s 19s/step - loss: 0.0215 - val_loss: 0.0386 - lr: 3.1250e-04\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0211 \n",
      "Epoch 23: val_loss improved from 0.03859 to 0.03669, saving model to best_model.h5\n",
      "34/34 [==============================] - 658s 19s/step - loss: 0.0211 - val_loss: 0.0367 - lr: 3.1250e-04\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0208 \n",
      "Epoch 24: val_loss improved from 0.03669 to 0.03505, saving model to best_model.h5\n",
      "34/34 [==============================] - 655s 19s/step - loss: 0.0208 - val_loss: 0.0351 - lr: 3.1250e-04\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0200 \n",
      "Epoch 25: val_loss improved from 0.03505 to 0.03451, saving model to best_model.h5\n",
      "34/34 [==============================] - 657s 19s/step - loss: 0.0200 - val_loss: 0.0345 - lr: 3.1250e-04\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0199 \n",
      "Epoch 26: val_loss improved from 0.03451 to 0.03344, saving model to best_model.h5\n",
      "34/34 [==============================] - 667s 19s/step - loss: 0.0199 - val_loss: 0.0334 - lr: 3.1250e-04\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0195 \n",
      "Epoch 27: val_loss improved from 0.03344 to 0.02836, saving model to best_model.h5\n",
      "34/34 [==============================] - 654s 19s/step - loss: 0.0195 - val_loss: 0.0284 - lr: 3.1250e-04\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0189 \n",
      "Epoch 28: val_loss did not improve from 0.02836\n",
      "34/34 [==============================] - 662s 19s/step - loss: 0.0189 - val_loss: 0.0299 - lr: 3.1250e-04\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0188 \n",
      "Epoch 29: val_loss did not improve from 0.02836\n",
      "34/34 [==============================] - 656s 19s/step - loss: 0.0188 - val_loss: 0.0290 - lr: 3.1250e-04\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0188 \n",
      "Epoch 30: val_loss improved from 0.02836 to 0.02443, saving model to best_model.h5\n",
      "34/34 [==============================] - 659s 20s/step - loss: 0.0188 - val_loss: 0.0244 - lr: 3.1250e-04\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0192 \n",
      "Epoch 31: val_loss improved from 0.02443 to 0.02144, saving model to best_model.h5\n",
      "34/34 [==============================] - 651s 19s/step - loss: 0.0192 - val_loss: 0.0214 - lr: 3.1250e-04\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0188 \n",
      "Epoch 32: val_loss improved from 0.02144 to 0.01986, saving model to best_model.h5\n",
      "34/34 [==============================] - 650s 19s/step - loss: 0.0188 - val_loss: 0.0199 - lr: 3.1250e-04\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0184 \n",
      "Epoch 33: val_loss did not improve from 0.01986\n",
      "34/34 [==============================] - 659s 19s/step - loss: 0.0184 - val_loss: 0.0223 - lr: 3.1250e-04\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0182 \n",
      "Epoch 34: val_loss did not improve from 0.01986\n",
      "34/34 [==============================] - 656s 19s/step - loss: 0.0182 - val_loss: 0.0250 - lr: 3.1250e-04\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0182 \n",
      "Epoch 35: val_loss did not improve from 0.01986\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "34/34 [==============================] - 649s 19s/step - loss: 0.0182 - val_loss: 0.0219 - lr: 3.1250e-04\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0179 \n",
      "Epoch 36: val_loss did not improve from 0.01986\n",
      "34/34 [==============================] - 651s 19s/step - loss: 0.0179 - val_loss: 0.0209 - lr: 1.5625e-04\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0171 \n",
      "Epoch 37: val_loss did not improve from 0.01986\n",
      "34/34 [==============================] - 652s 19s/step - loss: 0.0171 - val_loss: 0.0223 - lr: 1.5625e-04\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0170 \n",
      "Epoch 38: val_loss did not improve from 0.01986\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "34/34 [==============================] - 644s 19s/step - loss: 0.0170 - val_loss: 0.0236 - lr: 1.5625e-04\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0168 \n",
      "Epoch 39: val_loss improved from 0.01986 to 0.01978, saving model to best_model.h5\n",
      "34/34 [==============================] - 647s 19s/step - loss: 0.0168 - val_loss: 0.0198 - lr: 7.8125e-05\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0166 \n",
      "Epoch 40: val_loss did not improve from 0.01978\n",
      "34/34 [==============================] - 652s 19s/step - loss: 0.0166 - val_loss: 0.0211 - lr: 7.8125e-05\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0165 \n",
      "Epoch 41: val_loss did not improve from 0.01978\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "34/34 [==============================] - 651s 19s/step - loss: 0.0165 - val_loss: 0.0202 - lr: 7.8125e-05\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0162 \n",
      "Epoch 42: val_loss improved from 0.01978 to 0.01904, saving model to best_model.h5\n",
      "34/34 [==============================] - 648s 19s/step - loss: 0.0162 - val_loss: 0.0190 - lr: 3.9062e-05\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0163 \n",
      "Epoch 43: val_loss improved from 0.01904 to 0.01791, saving model to best_model.h5\n",
      "34/34 [==============================] - 648s 19s/step - loss: 0.0163 - val_loss: 0.0179 - lr: 3.9062e-05\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0159 \n",
      "Epoch 44: val_loss improved from 0.01791 to 0.01755, saving model to best_model.h5\n",
      "34/34 [==============================] - 644s 19s/step - loss: 0.0159 - val_loss: 0.0175 - lr: 3.9062e-05\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0165 \n",
      "Epoch 45: val_loss did not improve from 0.01755\n",
      "34/34 [==============================] - 645s 19s/step - loss: 0.0165 - val_loss: 0.0177 - lr: 3.9062e-05\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0158 \n",
      "Epoch 46: val_loss improved from 0.01755 to 0.01732, saving model to best_model.h5\n",
      "34/34 [==============================] - 649s 19s/step - loss: 0.0158 - val_loss: 0.0173 - lr: 3.9062e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0159 \n",
      "Epoch 47: val_loss improved from 0.01732 to 0.01688, saving model to best_model.h5\n",
      "34/34 [==============================] - 639s 19s/step - loss: 0.0159 - val_loss: 0.0169 - lr: 3.9062e-05\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0158 \n",
      "Epoch 48: val_loss improved from 0.01688 to 0.01558, saving model to best_model.h5\n",
      "34/34 [==============================] - 655s 19s/step - loss: 0.0158 - val_loss: 0.0156 - lr: 3.9062e-05\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0162 \n",
      "Epoch 49: val_loss improved from 0.01558 to 0.01543, saving model to best_model.h5\n",
      "34/34 [==============================] - 645s 19s/step - loss: 0.0162 - val_loss: 0.0154 - lr: 3.9062e-05\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0158 \n",
      "Epoch 50: val_loss improved from 0.01543 to 0.01538, saving model to best_model.h5\n",
      "34/34 [==============================] - 649s 19s/step - loss: 0.0158 - val_loss: 0.0154 - lr: 3.9062e-05\n",
      "Epoch 51/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0160 \n",
      "Epoch 51: val_loss improved from 0.01538 to 0.01513, saving model to best_model.h5\n",
      "34/34 [==============================] - 644s 19s/step - loss: 0.0160 - val_loss: 0.0151 - lr: 3.9062e-05\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0157 \n",
      "Epoch 52: val_loss did not improve from 0.01513\n",
      "34/34 [==============================] - 645s 19s/step - loss: 0.0157 - val_loss: 0.0167 - lr: 3.9062e-05\n",
      "Epoch 53/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0157 \n",
      "Epoch 53: val_loss did not improve from 0.01513\n",
      "34/34 [==============================] - 637s 19s/step - loss: 0.0157 - val_loss: 0.0167 - lr: 3.9062e-05\n",
      "Epoch 54/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0156 \n",
      "Epoch 54: val_loss did not improve from 0.01513\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "34/34 [==============================] - 653s 19s/step - loss: 0.0156 - val_loss: 0.0160 - lr: 3.9062e-05\n",
      "Epoch 55/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0155 \n",
      "Epoch 55: val_loss did not improve from 0.01513\n",
      "34/34 [==============================] - 638s 19s/step - loss: 0.0155 - val_loss: 0.0155 - lr: 1.9531e-05\n",
      "Epoch 56/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0154 \n",
      "Epoch 56: val_loss did not improve from 0.01513\n",
      "34/34 [==============================] - 646s 19s/step - loss: 0.0154 - val_loss: 0.0159 - lr: 1.9531e-05\n",
      "Epoch 57/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0152 \n",
      "Epoch 57: val_loss improved from 0.01513 to 0.01495, saving model to best_model.h5\n",
      "34/34 [==============================] - 644s 19s/step - loss: 0.0152 - val_loss: 0.0149 - lr: 1.9531e-05\n",
      "Epoch 58/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0156 \n",
      "Epoch 58: val_loss did not improve from 0.01495\n",
      "34/34 [==============================] - 638s 19s/step - loss: 0.0156 - val_loss: 0.0151 - lr: 1.9531e-05\n",
      "Epoch 59/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0148 \n",
      "Epoch 59: val_loss improved from 0.01495 to 0.01469, saving model to best_model.h5\n",
      "34/34 [==============================] - 657s 19s/step - loss: 0.0148 - val_loss: 0.0147 - lr: 1.9531e-05\n",
      "Epoch 60/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0156 \n",
      "Epoch 60: val_loss improved from 0.01469 to 0.01440, saving model to best_model.h5\n",
      "34/34 [==============================] - 642s 19s/step - loss: 0.0156 - val_loss: 0.0144 - lr: 1.9531e-05\n",
      "Epoch 61/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0149 \n",
      "Epoch 61: val_loss did not improve from 0.01440\n",
      "34/34 [==============================] - 649s 19s/step - loss: 0.0149 - val_loss: 0.0150 - lr: 1.9531e-05\n",
      "Epoch 62/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0155 \n",
      "Epoch 62: val_loss did not improve from 0.01440\n",
      "34/34 [==============================] - 645s 19s/step - loss: 0.0155 - val_loss: 0.0148 - lr: 1.9531e-05\n",
      "Epoch 63/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0149 \n",
      "Epoch 63: val_loss did not improve from 0.01440\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "34/34 [==============================] - 650s 19s/step - loss: 0.0149 - val_loss: 0.0150 - lr: 1.9531e-05\n",
      "Epoch 64/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0151 \n",
      "Epoch 64: val_loss did not improve from 0.01440\n",
      "34/34 [==============================] - 645s 19s/step - loss: 0.0151 - val_loss: 0.0144 - lr: 9.7656e-06\n",
      "Epoch 65/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0154 \n",
      "Epoch 65: val_loss did not improve from 0.01440\n",
      "34/34 [==============================] - 637s 19s/step - loss: 0.0154 - val_loss: 0.0144 - lr: 9.7656e-06\n",
      "Epoch 66/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0151 \n",
      "Epoch 66: val_loss improved from 0.01440 to 0.01427, saving model to best_model.h5\n",
      "34/34 [==============================] - 643s 19s/step - loss: 0.0151 - val_loss: 0.0143 - lr: 9.7656e-06\n",
      "Epoch 67/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0150 \n",
      "Epoch 67: val_loss improved from 0.01427 to 0.01421, saving model to best_model.h5\n",
      "34/34 [==============================] - 630s 19s/step - loss: 0.0150 - val_loss: 0.0142 - lr: 9.7656e-06\n",
      "Epoch 68/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0148 \n",
      "Epoch 68: val_loss improved from 0.01421 to 0.01397, saving model to best_model.h5\n",
      "34/34 [==============================] - 589s 17s/step - loss: 0.0148 - val_loss: 0.0140 - lr: 9.7656e-06\n",
      "Epoch 69/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0151 \n",
      "Epoch 69: val_loss did not improve from 0.01397\n",
      "34/34 [==============================] - 625s 18s/step - loss: 0.0151 - val_loss: 0.0141 - lr: 9.7656e-06\n",
      "Epoch 70/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0149 \n",
      "Epoch 70: val_loss did not improve from 0.01397\n",
      "34/34 [==============================] - 625s 19s/step - loss: 0.0149 - val_loss: 0.0144 - lr: 9.7656e-06\n",
      "Epoch 71/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0150 \n",
      "Epoch 71: val_loss did not improve from 0.01397\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "34/34 [==============================] - 640s 19s/step - loss: 0.0150 - val_loss: 0.0144 - lr: 9.7656e-06\n",
      "Epoch 72/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0149 \n",
      "Epoch 72: val_loss did not improve from 0.01397\n",
      "34/34 [==============================] - 633s 19s/step - loss: 0.0149 - val_loss: 0.0141 - lr: 4.8828e-06\n",
      "Epoch 73/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0150 \n",
      "Epoch 73: val_loss did not improve from 0.01397\n",
      "34/34 [==============================] - 629s 19s/step - loss: 0.0150 - val_loss: 0.0142 - lr: 4.8828e-06\n",
      "Epoch 74/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0150 \n",
      "Epoch 74: val_loss improved from 0.01397 to 0.01392, saving model to best_model.h5\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "34/34 [==============================] - 636s 19s/step - loss: 0.0150 - val_loss: 0.0139 - lr: 4.8828e-06\n",
      "Epoch 75/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0146 \n",
      "Epoch 75: val_loss improved from 0.01392 to 0.01385, saving model to best_model.h5\n",
      "34/34 [==============================] - 631s 18s/step - loss: 0.0146 - val_loss: 0.0138 - lr: 2.4414e-06\n",
      "Epoch 76/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0148 \n",
      "Epoch 76: val_loss improved from 0.01385 to 0.01376, saving model to best_model.h5\n",
      "34/34 [==============================] - 634s 19s/step - loss: 0.0148 - val_loss: 0.0138 - lr: 2.4414e-06\n",
      "Epoch 77/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0148 \n",
      "Epoch 77: val_loss did not improve from 0.01376\n",
      "34/34 [==============================] - 634s 19s/step - loss: 0.0148 - val_loss: 0.0138 - lr: 2.4414e-06\n",
      "Epoch 78/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0146 \n",
      "Epoch 78: val_loss did not improve from 0.01376\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 635s 19s/step - loss: 0.0146 - val_loss: 0.0138 - lr: 2.4414e-06\n",
      "Epoch 79/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0147 \n",
      "Epoch 79: val_loss did not improve from 0.01376\n",
      "34/34 [==============================] - 638s 19s/step - loss: 0.0147 - val_loss: 0.0138 - lr: 1.2207e-06\n",
      "Epoch 80/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0146 \n",
      "Epoch 80: val_loss did not improve from 0.01376\n",
      "34/34 [==============================] - 636s 19s/step - loss: 0.0146 - val_loss: 0.0138 - lr: 1.2207e-06\n",
      "Epoch 81/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0149 \n",
      "Epoch 81: val_loss did not improve from 0.01376\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "34/34 [==============================] - 637s 19s/step - loss: 0.0149 - val_loss: 0.0138 - lr: 1.2207e-06\n",
      "Epoch 82/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0145 \n",
      "Epoch 82: val_loss improved from 0.01376 to 0.01375, saving model to best_model.h5\n",
      "34/34 [==============================] - 640s 19s/step - loss: 0.0145 - val_loss: 0.0138 - lr: 6.1035e-07\n",
      "Epoch 83/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0147 \n",
      "Epoch 83: val_loss improved from 0.01375 to 0.01375, saving model to best_model.h5\n",
      "34/34 [==============================] - 636s 19s/step - loss: 0.0147 - val_loss: 0.0138 - lr: 6.1035e-07\n",
      "Epoch 84/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0149 \n",
      "Epoch 84: val_loss improved from 0.01375 to 0.01374, saving model to best_model.h5\n",
      "34/34 [==============================] - 642s 19s/step - loss: 0.0149 - val_loss: 0.0137 - lr: 6.1035e-07\n",
      "Epoch 85/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0146 \n",
      "Epoch 85: val_loss did not improve from 0.01374\n",
      "34/34 [==============================] - 648s 19s/step - loss: 0.0146 - val_loss: 0.0138 - lr: 6.1035e-07\n",
      "Epoch 86/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0148 \n",
      "Epoch 86: val_loss did not improve from 0.01374\n",
      "34/34 [==============================] - 653s 19s/step - loss: 0.0148 - val_loss: 0.0138 - lr: 6.1035e-07\n",
      "Epoch 87/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0146 \n",
      "Epoch 87: val_loss improved from 0.01374 to 0.01374, saving model to best_model.h5\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "34/34 [==============================] - 647s 19s/step - loss: 0.0146 - val_loss: 0.0137 - lr: 6.1035e-07\n",
      "Epoch 88/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0147 \n",
      "Epoch 88: val_loss did not improve from 0.01374\n",
      "34/34 [==============================] - 659s 19s/step - loss: 0.0147 - val_loss: 0.0137 - lr: 3.0518e-07\n",
      "Epoch 89/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0146 \n",
      "Epoch 89: val_loss improved from 0.01374 to 0.01373, saving model to best_model.h5\n",
      "34/34 [==============================] - 648s 19s/step - loss: 0.0146 - val_loss: 0.0137 - lr: 3.0518e-07\n",
      "Epoch 90/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0146 \n",
      "Epoch 90: val_loss did not improve from 0.01373\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "34/34 [==============================] - 658s 19s/step - loss: 0.0146 - val_loss: 0.0137 - lr: 3.0518e-07\n",
      "Epoch 91/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0147 \n",
      "Epoch 91: val_loss improved from 0.01373 to 0.01373, saving model to best_model.h5\n",
      "34/34 [==============================] - 667s 20s/step - loss: 0.0147 - val_loss: 0.0137 - lr: 1.5259e-07\n",
      "Epoch 92/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0145 \n",
      "Epoch 92: val_loss did not improve from 0.01373\n",
      "34/34 [==============================] - 663s 20s/step - loss: 0.0145 - val_loss: 0.0137 - lr: 1.5259e-07\n",
      "Epoch 93/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0149 \n",
      "Epoch 93: val_loss did not improve from 0.01373\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "34/34 [==============================] - 657s 19s/step - loss: 0.0149 - val_loss: 0.0137 - lr: 1.5259e-07\n",
      "Epoch 94/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0148 \n",
      "Epoch 94: val_loss improved from 0.01373 to 0.01373, saving model to best_model.h5\n",
      "34/34 [==============================] - 670s 20s/step - loss: 0.0148 - val_loss: 0.0137 - lr: 1.0000e-07\n",
      "Epoch 95/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0145 \n",
      "Epoch 95: val_loss did not improve from 0.01373\n",
      "34/34 [==============================] - 671s 20s/step - loss: 0.0145 - val_loss: 0.0137 - lr: 1.0000e-07\n",
      "Epoch 96/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0146 \n",
      "Epoch 96: val_loss did not improve from 0.01373\n",
      "34/34 [==============================] - 674s 20s/step - loss: 0.0146 - val_loss: 0.0137 - lr: 1.0000e-07\n",
      "Epoch 97/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0148 \n",
      "Epoch 97: val_loss did not improve from 0.01373\n",
      "34/34 [==============================] - 674s 20s/step - loss: 0.0148 - val_loss: 0.0137 - lr: 1.0000e-07\n",
      "Epoch 98/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0146 \n",
      "Epoch 98: val_loss improved from 0.01373 to 0.01373, saving model to best_model.h5\n",
      "34/34 [==============================] - 667s 20s/step - loss: 0.0146 - val_loss: 0.0137 - lr: 1.0000e-07\n",
      "Epoch 99/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0147 \n",
      "Epoch 99: val_loss improved from 0.01373 to 0.01373, saving model to best_model.h5\n",
      "34/34 [==============================] - 688s 20s/step - loss: 0.0147 - val_loss: 0.0137 - lr: 1.0000e-07\n",
      "Epoch 100/100\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0147 \n",
      "Epoch 100: val_loss did not improve from 0.01373\n",
      "34/34 [==============================] - 685s 20s/step - loss: 0.0147 - val_loss: 0.0137 - lr: 1.0000e-07\n"
     ]
    }
   ],
   "source": [
    "#Batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Input shape\n",
    "input_shape = (600, 264, 1)\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Build the autoencoder\n",
    "encoded = encoder(inputs)\n",
    "decoded = decoder(encoded)\n",
    "\n",
    "autoencoder = Model(inputs, decoded)\n",
    "\n",
    "# Optimizer adapts learning rate based on batch size, using Adam optimizer\n",
    "optimizer = Adam(learning_rate=0.01*(batch_size/256), beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Summary of the model\n",
    "autoencoder.summary()\n",
    "\n",
    "# Initialize model checkpointing to save the model with the lowest loss.\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Set up early stopping to halt training if loss doesn't improve after 20 epochs.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, mode='min', verbose=1)\n",
    "\n",
    "# Configure learning rate reduction if loss doesn't improve after 3 epochs.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, factor=0.5, min_lr=0.0000001)\n",
    "\n",
    "# Fit the model\n",
    "history = autoencoder.fit(generator(X_train_normalized, y_train_normalized),\n",
    "                          steps_per_epoch=len(X_train_normalized) // batch_size,\n",
    "                          epochs=100,\n",
    "                          validation_data=(X_val_normalized, y_val_normalized),\n",
    "                          callbacks=[model_checkpoint, early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9392840",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52009dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'best_model.h5' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Network visualization\n",
    "netron.start('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f758a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "490582da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 156s 9s/step - loss: 0.0138\n",
      "3/3 [==============================] - 15s 4s/step - loss: 0.0137\n",
      "3/3 [==============================] - 24s 7s/step - loss: 0.0141\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013817</td>\n",
       "      <td>0.013727</td>\n",
       "      <td>0.014147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      train  validation      test\n",
       "0  0.013817    0.013727  0.014147"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train loss\n",
    "train_loss = autoencoder.evaluate(X_train_normalized, y_train_normalized, verbose=1)\n",
    "\n",
    "#Validation loss\n",
    "val_loss = autoencoder.evaluate(X_val_normalized, y_val_normalized, verbose=1)\n",
    "\n",
    "#Test loss\n",
    "test_loss = autoencoder.evaluate(X_test_normalized, y_test_normalized, verbose=1)\n",
    "\n",
    "results = pd.DataFrame([[train_loss, val_loss, test_loss]],\n",
    "            columns = ['train', 'validation', 'test'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6380a3cc",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d816107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_predictions(output_prediction, phase, global_min=global_min, global_max=global_max):\n",
    "    prediction = denormalize(output_prediction.squeeze(), global_min, global_max)\n",
    "    audio_prediction = stft_to_audio(prediction, phase, hop_length_fft=304)\n",
    "    return audio_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d031f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder=load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8aebbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 19s 4s/step\n"
     ]
    }
   ],
   "source": [
    "predictions = autoencoder.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d116426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_pred = get_audio_predictions(predictions, phase_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06593e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save all the predictions for test data\n",
    "for i, element in enumerate(audio_pred):\n",
    "    sf.write(f'audio_pred{i}.wav', audio_pred[i], samplerate=sr)\n",
    "    sf.write(f'audio_noisy{i}.wav', stft_to_audio(X_test[i], phase_test[i], hop_length_fft=304), samplerate=sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
